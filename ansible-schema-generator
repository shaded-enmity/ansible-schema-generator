#!/usr/bin/env python

# ansible-schema-generator
#
# Copyright (C) 2017 Pavel Odvody <podvody@redhat.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import ast
import argparse
import imp
import json
import logging
import os
import re
import sys
from collections import defaultdict, namedtuple, OrderedDict

import daiquiri
import semver
import yaml


logger = daiquiri.getLogger('ansible-schema')


DOCUMENTATION_KEY = 'DOCUMENTATION'
TITLE_KEYS = ('module',)
IGNORE_KEYS = (('plugin_type',), ('strategy',), ('callback'),)
FREE_FORM_KEYS = (('free_form',), ('free-form',),)
COMMAND_MODULES = ('shell', 'command', 'script', 'raw',)
META_MODULES = ('include_vars', 'include', 'import_tasks', 'include_tasks', 'import_playbook',)
LOG_LEVEL_MAP = {'debug': logging.DEBUG,
                 'warning': logging.WARNING, 'error': logging.ERROR}
LIST_MATCHER = re.compile(r'^\W*[Ll]ist')
DICT_MATCHER = re.compile(r'\W*[Aa]? ?[Dd]ictionary')


ANSIBLE_NUMBER_LIKE = {'oneOf': [{'type': 'integer'}, {'type': 'string'}]}
ANSIBLE_TRUTH_LIKE = {'oneOf': [{'type': 'boolean'},
                                {'enum': [0, 1], 'type': 'integer'},
                                {'enum': ['yes',
                                          'no',
                                          'Yes',
                                          'No',
                                          'YES',
                                          'NO',
                                          'on',
                                          'off',
                                          'On',
                                          'Off',
                                          'ON',
                                          'OFF',
                                          '1',
                                          '0',
                                          'true',
                                          'false',
                                          'True',
                                          'False'
                                          'TRUE',
                                          'FALSE'], 'type': 'string'}]}
ANSIBLE_ARRAY_LIKE = {'oneOf': [{'type': 'string'}, {'type': 'array'}, {'type': 'object'}]} #should we be more relaxed here?


Property = namedtuple('Property', 'name description type required aliases choices')
TypeFix = namedtuple('TypeFix', 'module option type version')


def import_function_from_file(file, func):
    ''' This is a hack which pollutes global state, use ~wisely~, ~in rare cases~, never! '''
    code = compile(open(file).read(), file, 'exec')
    exec(code, globals(), globals())
    return globals()[func]


def organize_type_fixes(typefixes):
    fixes = defaultdict(dict)
    for tf in typefixes:
        fixes[tf.module][tf.option] = (tf.version, tf.type)
    return fixes

__typefixes = organize_type_fixes([])


def apply_fix(fix, default, version):
    version_match, alt_type = fix
    if semver.match(version, version_match):
        return alt_type
    return default


def get_definitions():
    matcher = re.compile('^ANSIBLE_([A-Z]*)_LIKE$')
    g = globals()
    defs = {}
    for n, v in g.iteritems():
        m = matcher.match(n)
        if m:
            name = m.group(1).lower()
            defs['ansible_' + name] = v
    return defs


class DuplicateNameError(Exception):
    def __init__(self, duplicates):
        self.duplicates = duplicates


class NoRequirements(Exception):
    pass


class PropertyList(list):
    def _get_template(self):
        return {
            'properties': [],
            'allOf': []
        }


    def _flatten_property_names(self):
        ''' Flatten property and alias names into a single list '''
        for prop in self:
            yield {'name': prop.name, 'type': prop.type, 'desc': prop.description, 'choices': prop.choices}
            for alias in prop.aliases:
                yield {'name': alias, 'type': prop.type, 'desc': prop.description, 'choices': prop.choices}


    def _find_duplicate_names(self):
        ''' Find duplicates to report'''
        props = list(self._flatten_property_names())
        as_set = {p['name'] for p in props}
        if len(as_set) != len(props):
            seen = set()
            repeated = []
            for e in props:
                if e['name'] in seen:
                    repeated.append(e['name'])
                seen.add(e['name'])
            return repeated
        return False


    def get_json_schema_key_values(self, strict=False):
        ''' Generate the actual schema fields to be merged into the parent schema

            Raises `DuplicateNameError` when properties contain duplicate entries
            Raises `NoRequirements` when `strict=True` and the object has 0 required properties
        '''
        duplicates = self._find_duplicate_names()
        if duplicates:
            raise DuplicateNameError(duplicates)

        # Four possible cases here:
        #
        #   1) Required, no aliases
        #   2) Required, aliases
        #   3) Non required, no aliases
        #   4) Non required, aliases
        #

        schema = self._get_template()
        mutexes = []

        for prop in self:
            if prop.aliases:
                all_names = [prop.name] + prop.aliases
                mutex = {
                    'allOf': [{
                        'not': {
                            'required': all_names,
                            'type': 'object'
                        },
                    }]
                }
                if prop.required:
                    mutex['allOf'].append({
                        'oneOf': [
                          {'required': [n], 'type': 'object'}  for n in all_names
                        ]
                    })
                mutexes.append(mutex)

        def _unwrap_type(p):
            if isinstance(p['type'], dict):
                copy = p['type'].copy()
                copy['description'] = p['desc']
                return copy
            else:
                base = {'type': p['type'], 'description': p['desc']}
                if p['choices']:
                    if _are_choices_clean(p['choices']):
                        base['enum'] = p['choices']
                        base = {'anyOf': [base, {'type': 'string'}]}
                    else:
                        logger.warning('Passing `choices`: {} via description'.format(p['choices']))
                        base['description'] += '\n\nPossible choices:\n' + '\n'.join(map(str, p['choices']))
                return base

        schema['properties'] = {
            p['name']: _unwrap_type(p) for p in self._flatten_property_names()
        }

        if mutexes:
            schema['allOf'] = mutexes
        else:
            del schema['allOf']
            req = [p.name for p in self if p.required]
            if req:
                schema['required'] = req
            elif strict:
                raise NoRequirements

        return schema


def _are_choices_clean(choices):
    ''' Sanity check to make sure that there are no extra choices expected '''
    for choice in choices:
        str_choice = str(choice)
        if ' ' in str_choice or '\n' in str_choice:
            return False
    try:
        # Sometimes we get [['some_string]] which won't work
        # as lists are not hashable
        return len(set(choices)) == len(choices)
    except TypeError:
        return False


def _find_script_files(path):
    ''' Find all files ending with `.py` in the given directory tree and return them '''
    target_directory = os.path.abspath(path)
    script_files, core_files = [], []

    for root, _, files in os.walk(target_directory):
        for file in files:
            absolute_path = os.path.join(root, file)
            # Skip symlinks so that we don't process the same file more than once
            if os.path.islink(absolute_path):
                continue
            if file.endswith('.py'):
                if '/lib/ansible/modules/core' in absolute_path:
                    core_files.append(absolute_path)
                else:
                    script_files.append(absolute_path)

    return script_files + core_files


def _parse_files(files):
    ''' Parse Python files and return a mapping of `path` => `ast.Module` '''
    parsed_files = OrderedDict()
    for file in files:
        with open(file, 'r') as fp:
            try:
                parsed_files[file] = ast.parse(fp.read(), file)
            except:
                logger.warning('Invalid Python file {}'.format(file))
                continue
    return parsed_files


def _find_documentation_variable(filename, module):
    ''' Extract `DOCUMENTATION` string from the module '''
    for node in ast.iter_child_nodes(module):
        if isinstance(node, ast.Assign) and isinstance(node.targets[0], ast.Name):
            if node.targets[0].id == DOCUMENTATION_KEY:
                # We could probably dig a little but deeper here, this seems to
                # suffice though
                if not isinstance(node.value, ast.Str):
                    logger.warning('Value for {} is not a string but "{}"'.format(
                        filename, type(node.value)))

                try:
                    return yaml.load(node.value.s)
                except yaml.YAMLError:
                    logger.error('Error loading {}'.format(filename))
                    return None


def _convert(type_name):
    TYPE_MAP = {
            'int': {'$ref': '#/definitions/ansible_number'},
            'bool': {'$ref': '#/definitions/ansible_truth'},
            'str': 'string',
            'float': 'number',
            'object': 'object',
            'dict': 'object',
            'list': {'$ref': '#/definitions/ansible_array'},
            'path': 'string'
    }
    return TYPE_MAP.get(type_name, 'string')

def _infer_type(option, module, name, version):
    ''' Infer JSON schema types from Ansible type description

        returns tuple (required, type_descriptor)
    '''
    default = option.get('default', None)
    description = option.get('description', '')
    if isinstance(description, list):
        description = "\n".join(description)

    type_descriptor = {}

    choices = option.get('choices', None)

    mapped_type = _convert(option.get('type'))
    # This is a hack, however there doesn't seem any better way
    # of distinguishing whether the given option is a list option
    # so handle it heuristically
    if LIST_MATCHER.match(description):
        type_descriptor = {'type': 'array', 'items': {'type': mapped_type}}
    elif DICT_MATCHER.match(description):
        type_descriptor = {'type': 'object'}
    else:
        if isinstance(mapped_type, dict):
            type_descriptor = mapped_type
        else:
            type_descriptor = {'type': mapped_type}
            if mapped_type == 'string' and choices:
                type_descriptor['choices'] = choices

    if module in __typefixes:
        module_specific = __typefixes[module]
        if name in module_specific:
            rspec = module_specific[name]
            type_descriptor = apply_fix(rspec, type_descriptor, version)

    if default:
        description = 'Default: ' + str(default) + '\n\n' + description
    type_descriptor['description'] = description

    return (option.get('required', False), type_descriptor)


def _take_key(key_list, obj):
    ''' Iterate over keys in `key_list` and return the first one which
        is present in `obj`, or None
    '''
    for key in key_list:
        if key in obj:
            return obj[key]

    return None


def _check_keys(key_set_list, obj):
    ''' Check if `obj` contains a full `key_set` from `key_set_list` '''
    for key_set in key_set_list:
        if all(key in obj for key in key_set):
            return True

    return False


def _get_top_level_functions(tree):
    ''' Get a list of top level function names '''
    for node in ast.iter_child_nodes(tree):
        if isinstance(node, ast.FunctionDef):
            yield node.name


def _validate_docs_argument_spec(items, reporter):
    ''' Yield one item for each tuple in `items` and report
        in case of inequality between A/B

    '''
    for (a, b, comparison, kind) in items:
        try:
            if comparison:
                if comparison(a) == comparison(b):
                    yield a
                else:
                    reporter(a, b, kind)
                    yield b
            else:
                if a == b:
                    yield a
                else:
                    reporter(a, b, kind)
                    yield b
        except:
            yield None
            reporter(a, b, 'CONVERSION')


def _pick_values(items, reporter):
    ''' Picks the A/B values from `items` which contains data about:

        1) Documentation spec
        2) Argument spec

        While in most cases the latter should be used, some internal modules
        do not expose argument_spec and only expose documentation
    '''
    return tuple(_validate_docs_argument_spec(items, reporter))


def _tranform_deferred_schemas(options, version, argspec=None):
    ''' Deferred transformation to handle particular `free_form` commands '''
    schemas = []
    for (filename, contents, is_command) in options:
        props, requireds = {}, []
        title = _take_key(TITLE_KEYS, contents)
        if title is None:
            logger.warning('No object title for: {}'.format(filename))
            continue

        if is_command:
            schema = {'name': {'type': 'string'}, 'args': {
                      'type': 'object', 'properties': props, 'required': requireds}}
        else:
            requireds.append(title)
            schema = {'name': {'type': 'string'}, title: {'type': 'string'},
                      'properties': {}, 'required': requireds}

        properties = PropertyList()
        data = contents.get('options') or {}
        for key, value in data.iteritems():
            # free form is only an indicator of the type of command
            # and should not be an actual key to chooser froma
            if _check_keys(FREE_FORM_KEYS, [key]):
                continue
            aliases = value.get('aliases', [])
            if isinstance(aliases, str):
                aliases = [aliases]

            # since none of the free form modules currently expose their Argspec
            # we have to infer properties from the documentation strings
            required, descriptor = _infer_type(value, title, key, version)
            typ = descriptor['type'] if 'type' in descriptor else descriptor
            choices = descriptor['choices'] if 'choices' in descriptor else None

            properties.append(
                Property(key, descriptor['description'], typ, required, aliases, choices)
            )

            if required:
                requireds.append(key)

        description = contents.get('description', '')
        if isinstance(description, list):
            description = "\n".join(description)

        try:
            if is_command:
                schema['args'].update(properties.get_json_schema_key_values())
            else:
                schema.update(properties.get_json_schema_key_values())
        except DuplicateNameError as e:
            logger.error(
                'Skipping {} because it contains duplicate property keys:\n{}'.format(filename, e.duplicates)
            )
            continue
        except NoRequirements:
            logger.error(
                'Skipping {} because no properties are required'.format(filename)
            )
            continue

        schema[title] = {'type': 'string'}

        if not requireds:
            del schema['args']['required']

        if is_command:
            schemas.append({'type': 'object', 'properties': schema,
                            'required': [title]})
        else:
            schemas.append(schema)

    return schemas


def _transform_options_to_schemas(options, deferred, version, argspec=None, report=None):
    ''' Transform Ansible `options` specification into JSON descriptors '''
    schemas = {'name': {'type': 'string'}, 'blocks': {'type': {'$ref': '#/definitions/ansible_block'}}}
    for (filename, contents, tree) in options:
        if _check_keys(IGNORE_KEYS, contents):
            logger.debug('Ignoring source file: {}'.format(filename))
            continue

        schema = {'type': 'object', 'description': None, 'properties': dict()}

        title = _take_key(TITLE_KEYS, contents)
        if title is None:
            logger.warning('No object title for: {}'.format(filename))
            continue
        elif title in schemas:
            report.append({'kind': 'duplicate_module', 'module': title, 'file': filename})
            continue

        data = contents.get('options') or {}
        if _check_keys(FREE_FORM_KEYS, data):
            if title in COMMAND_MODULES:
                deferred.append((filename, contents, True))
                continue
            elif title in META_MODULES:
                deferred.append((filename, contents, False))
                continue
            else:
                logger.warning(
                    'Ignoring non-command free form module {}'.format(filename))
                continue

        spec = {}
        if 'main' in set(_get_top_level_functions(tree)):
            # Perform argspec inspection only on things that actually implement
            # the main() function, otherwise it returns total rubish
            try:
                spec = argspec(filename)
            except:
                report.append({'kind': 'argspec_fail', 'file': filename})

        arguments = spec.get('argument_spec', {})
        if title != 'async_status':
            if 'jid' in arguments and 'mode' in arguments and len(arguments.keys()) == 2:
                del arguments['jid']
                del arguments['mode']

        keys = arguments.keys() or data.keys()

        # What are the other interesting keys that we can use here?
        #
        # check_mode = spec.get('supports_check_mode', False)
        # mutually_exclusive = spec.get('mutually_exclusive', [])
        #
        properties = PropertyList()
        for key in keys:
            st_desc = arguments.get(key, {})
            value = data.get(key, None)

            if not value:
                # documentation key does not exist, try aliases
                for al in st_desc.get('aliases', []):
                    value = data.get(al, None)
                    if value:
                        break

                if not value:
                    value = {}

            required, descriptor = _infer_type(value, title, key, version)
            aliases = value.get('aliases', [])

            if isinstance(aliases, str):
                aliases = [aliases]

            typ = descriptor['type'] if 'type' in descriptor else descriptor['$ref'] if '$ref' in descriptor else descriptor
            choices = descriptor['choices'] if 'choices' in descriptor else []

            stype = _convert(st_desc.get('type', 'str'))
            schoices = st_desc.get('choices', [])
            saliases = st_desc.get('aliases', [])
            srequired = st_desc.get('required', False)

            aliases, choices, typ, required = _pick_values(
               [(aliases, saliases, set, 'aliases'),
                (choices, schoices, set, 'choices'),
                (typ, stype, None, 'type'),
                (required, srequired, None, 'required')],
                lambda a, b, kind: report.append(
                    {'in_doc': str(a), 'in_argspec': str(b), 'kind': kind, 'key': key, 'file': filename}
                )
            )

            properties.append(
                Property(key, descriptor['description'], typ, required, list(aliases) if aliases else [], list(choices) if choices else [])
            )


        description = contents.get('description', '')
        if isinstance(description, list):
            description = "\n".join(description)

        schema['description'] = description

        try:
            schema.update(properties.get_json_schema_key_values())
        except DuplicateNameError as e:
            report.append({'kind': 'duplicate_keys', 'file': filename, 'duplicates': e.duplicates})
            continue
        except NoRequirements:
            report.append({'kind': 'zero_requirements', 'file': filename})
            continue

        report.append({'kind': 'success', 'module': title, 'file': filename})
        schemas[title] = schema

    return schemas


def _parse_arguments():
    description = ('Generate JSON schema for language servers from Ansible module' +
                   ' documentation in an Ansible repository checkout `target_path`')
    ap = argparse.ArgumentParser(
        prog='ansible-schema-generator', description=description)
    ap.add_argument('-o', '--output-file', default=None,
                    help='write result to this file')
    ap.add_argument('-l', '--log-level',
                    choices=['debug', 'warning', 'error'], default='warning', help='set log level')
    ap.add_argument('-d', '--description', default='Auto-Generated', help='Description string to add to the schema')
    ap.add_argument('-t', '--title', default='Ansible', help='Title of the schema')
    ap.add_argument('-v', '--version', default='2.5.0', help='Version of Ansible for which schemas are generated')
    ap.add_argument(
        'target_path', help='scan this directory for Ansible modules')
    ap.add_argument('-a', '--argspec-path',
        help='File from which we can import `get_argument_spec` function',
        default=None)
    ap.add_argument('-r', '--report-file', help='write report to this file', default=None)

    return ap.parse_args()


def main():
    arguments = _parse_arguments()
    target_path = arguments.target_path
    log_level = LOG_LEVEL_MAP[arguments.log_level]

    daiquiri.setup(level=log_level, program_name='ansible-schema')

    logger.debug('Loading modules from: {}'.format(
        os.path.abspath(target_path)))

    # determine where we should write the resulting data
    fp = sys.stdout
    if arguments.output_file:
        logger.debug('Setting output file to: {}'.format(
            os.path.abspath(arguments.output_file)))
        fp = open(arguments.output_file, 'w')

    # find python files and get their AST
    script_files = _find_script_files(target_path)
    parsed_files = _parse_files(script_files)

    # extract data from AST
    collected_data = []
    for filename, module in parsed_files.iteritems():
        variable_value = _find_documentation_variable(filename, module)
        if variable_value:
            collected_data.append((filename, variable_value, module))

    logger.debug('Transforming {} documents'.format(len(collected_data)))

    _argspec = import_function_from_file(
        arguments.argspec_path or os.getenv('ASG_ARGSPEC_PATH', None),
        'get_argument_spec'
    )
    argspec = lambda path: _argspec(path)[-1]

    reports = []
    deferred = []
    transformed = _transform_options_to_schemas(
        collected_data, deferred, arguments.version, argspec, reports
    )
    #sys.exit(0)

    if arguments.report_file:
        with open(arguments.report_file, 'w') as rf:
            json.dump(reports, rf, indent=3)
    #sys.exit(0)

    logger.debug('Handing {} deferred schemas'.format(len(deferred)))

    block_schema = {
        'type': 'object',
        'properties': {
            'block':  {'type': 'array', 'items': {'$ref': '#/definitions/ansible_role_block'}},
            'always': {'type': 'array', 'items': {'$ref': '#/definitions/ansible_role_block'}},
            'rescue': {'type': 'array', 'items': {'$ref': '#/definitions/ansible_role_block'}}
        }
    }

    transformed_deferred = _tranform_deferred_schemas(deferred, arguments.version, argspec)

    combined = {'anyOf': transformed_deferred + [{'type': 'object', 'properties': transformed}]}

    definitions = get_definitions()
    definitions['ansible_role_block'] = combined
    definitions['ansible_block'] = block_schema

    '''
     Use cases of validation:

     1) roles - where each module is defined as top-level object
        - module:
           args
     2) playbooks - where each module is defined as child of .tasks
        - something
          tasks:
     3) block, always, rescue - 
    '''
    validating_schemas = {'anyOf': [
        {'$ref': '#/definitions/ansible_role_block'},
        {'type': 'object', 'required': ['tasks'], 'properties': 
            {'tasks': {'anyOf': [{"type": "array", "items": {'$ref': '#/definitions/ansible_role_block'}}, {'$ref': '#/definitions/ansible_block'}]}}
        }
    ]}


    output_schemas = {'$schema': 'http://json-schema.org/draft-04/schema#',
                      'type': 'array', 'items': validating_schemas,
                      'description': arguments.description,
                      'title': arguments.title,
                      'definitions': definitions,
                      '$contact': {
                          'issues': 'https://github.com/shaded-enmity/ansible-schema-generator/issues',
                          'name': 'Pavel Odvody'
                      }}

    logger.debug('Transformation done!')
    logger.debug('Writing output')

    with fp:
        json.dump(output_schemas, fp, indent=3)

    logger.debug('Done')


if __name__ == '__main__':
    main()
